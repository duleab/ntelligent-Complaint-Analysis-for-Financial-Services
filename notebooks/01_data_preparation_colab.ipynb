# Data Preparation and Vector Store Creation

# Install required packages
!pip install -q transformers sentence-transformers langchain langchain-community faiss-cpu pandas numpy tqdm

import os
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import time
from tqdm.notebook import tqdm
import json

# Mount Google Drive to save results
from google.colab import drive
drive.mount('/content/drive')

# Set up paths
PROJECT_DIR = '/content/drive/MyDrive/FinancialComplaints'
os.makedirs(PROJECT_DIR, exist_ok=True)

# Download and extract data
!wget -q https://files.consumerfinance.gov/ccdb/complaints.csv.zip
!unzip -q complaints.csv.zip

# Load and preprocess data
def load_and_preprocess_data():
    """Load and preprocess the complaint data."""
    print("Loading and preprocessing data...")
    
    # Load data in chunks to handle large file
    chunk_size = 100000
    chunks = []
    
    for chunk in pd.read_csv('complaints.csv', chunksize=chunk_size):
        # Filter for records with narratives
        filtered_chunk = chunk.dropna(subset=['Consumer complaint narrative'])
        # Filter for minimum length
        filtered_chunk = filtered_chunk[filtered_chunk['Consumer complaint narrative'].str.len() > 50]
        # Keep relevant columns
        filtered_chunk = filtered_chunk[['Complaint ID', 'Consumer complaint narrative', 'Product', 'Issue', 'Company', 'Date received']]
        chunks.append(filtered_chunk)
        
        # Process first 1M records for demo
        if len(pd.concat(chunks)) > 1000000:
            break
    
    df = pd.concat(chunks)
    print(f"Total records processed: {len(df):,}")
    return df

# Text processing and chunking
def create_chunks(df, chunk_size=512):
    """Create chunks from text data."""
    print("\nCreating text chunks...")
    chunks = []
    chunk_metadata = []
    
    for _, row in tqdm(df.iterrows(), total=len(df)):
        text = row['Consumer complaint narrative']
        # Split text into chunks
        words = text.split()
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk) > 50:  # Minimum chunk length
                chunks.append(chunk)
                chunk_metadata.append({
                    'complaint_id': row['Complaint ID'],
                    'product': row['Product'],
                    'issue': row['Issue'],
                    'company': row['Company'],
                    'date': row['Date received']
                })
    
    print(f"Created {len(chunks):,} chunks")
    return chunks, chunk_metadata

# Create vector store
def create_vector_store(chunks, chunk_metadata):
    """Create FAISS vector store."""
    print("\nCreating vector store...")
    
    # Initialize model
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')
    
    # Create FAISS index
    embedding_dim = 384
    nlist = 100
    quantizer = faiss.IndexFlatL2(embedding_dim)
    index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)
    
    # Process in batches
    batch_size = 1000
    num_batches = len(chunks) // batch_size
    
    for i in tqdm(range(num_batches), desc="Processing batches"):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, len(chunks))
        
        # Get batch
        batch = chunks[start_idx:end_idx]
        
        # Get embeddings
        embeddings = model.encode(batch, batch_size=32, show_progress_bar=False)
        
        # Add to FAISS index
        if not index.is_trained:
            index.train(embeddings)
        index.add(embeddings)
    
    # Save index
    faiss.write_index(index, os.path.join(PROJECT_DIR, 'complaints_vector_store.index'))
    
    # Save metadata
    with open(os.path.join(PROJECT_DIR, 'chunk_metadata.json'), 'w') as f:
        json.dump(chunk_metadata, f)
    
    print("Vector store created successfully!")

# Main execution
def main():
    print("Starting data preparation pipeline...")
    
    # Step 1: Load and preprocess data
    df = load_and_preprocess_data()
    
    # Step 2: Create chunks
    chunks, chunk_metadata = create_chunks(df)
    
    # Step 3: Create vector store
    create_vector_store(chunks, chunk_metadata)
    
    print("\nPipeline completed successfully!")
    print(f"Results saved to: {PROJECT_DIR}")

if __name__ == "__main__":
    main()
