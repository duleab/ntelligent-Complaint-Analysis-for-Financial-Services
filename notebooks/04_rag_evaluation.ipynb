{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive RAG System Evaluation\n",
    "\n",
    "This notebook builds and evaluates a Retrieval-Augmented Generation (RAG) pipeline with enhanced metrics and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\programs\\lib\\site-packages (1.7.0)\n",
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\programs\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\programs\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\programs\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\programs\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: nltk in d:\\programs\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\programs\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in d:\\programs\\lib\\site-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programs\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\programs\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\programs\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=19457051722dbd68c65b71e841ef0e7ee3efe352b1e6cf42e18d20ca161049f6\n",
      "  Stored in directory: c:\\users\\villian\\appdata\\local\\pip\\cache\\wheels\\44\\af\\da\\5ffc433e2786f0b1a9c6f458d5fb8f611d8eb332387f18698f\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "\n",
      "   ---------------------------------------- 0/2 [absl-py]\n",
      "   ---------------------------------------- 0/2 [absl-py]\n",
      "   ---------------------------------------- 0/2 [absl-py]\n",
      "   ---------------------------------------- 0/2 [absl-py]\n",
      "   ---------------------------------------- 0/2 [absl-py]\n",
      "   ---------------------------------------- 0/2 [absl-py]\n",
      "   -------------------- ------------------- 1/2 [rouge-score]\n",
      "   -------------------- ------------------- 1/2 [rouge-score]\n",
      "   -------------------- ------------------- 1/2 [rouge-score]\n",
      "   ---------------------------------------- 2/2 [rouge-score]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 rouge-score-0.1.2\n",
      "Collecting bert-score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch in d:\\programs\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in d:\\programs\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: pandas>=1.0.1 in d:\\programs\\lib\\site-packages (from bert-score) (2.3.0)\n",
      "Requirement already satisfied: numpy in d:\\programs\\lib\\site-packages (from bert-score) (2.2.6)\n",
      "Requirement already satisfied: requests in d:\\programs\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in d:\\programs\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in d:\\programs\\lib\\site-packages (from bert-score) (3.10.3)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\programs\\lib\\site-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: filelock in d:\\programs\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\programs\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programs\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\programs\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\programs\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\programs\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\programs\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\programs\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programs\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programs\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\programs\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\programs\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programs\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programs\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\programs\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programs\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programs\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\programs\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programs\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\programs\\lib\\site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programs\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programs\\lib\\site-packages (from matplotlib->bert-score) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\programs\\lib\\site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\programs\\lib\\site-packages (from matplotlib->bert-score) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\programs\\lib\\site-packages (from matplotlib->bert-score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programs\\lib\\site-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\lib\\site-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\lib\\site-packages (from requests->bert-score) (2025.4.26)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n",
      "Requirement already satisfied: plotly in d:\\programs\\lib\\site-packages (6.1.2)\n",
      "Requirement already satisfied: pandas in d:\\programs\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in d:\\programs\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in d:\\programs\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in d:\\programs\\lib\\site-packages (from plotly) (1.42.0)\n",
      "Requirement already satisfied: packaging in d:\\programs\\lib\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\programs\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programs\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programs\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\programs\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\programs\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programs\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programs\\lib\\site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\programs\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\programs\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\programs\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programs\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# First install these core packages\n",
    "!pip install scikit-learn rouge-score\n",
    "\n",
    "# Then install bert-score with its specific requirements\n",
    "!pip install bert-score torch transformers\n",
    "\n",
    "# Finally install visualization packages\n",
    "!pip install plotly pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.schema import Document\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import HTML, display\n",
    "import plotly.express as px\n",
    "\n",
    "# Config\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Constants\n",
    "CONFIG = {\n",
    "    \"vector_store_dir\": \"../vector_store\",\n",
    "    \"results_dir\": \"../results\",\n",
    "    \"eval_data_path\": \"../data/evaluation_questions.json\",\n",
    "    \"llm_model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"retrieval_k\": 5,\n",
    "    \"evaluation_metrics\": [\"rouge\", \"bert_score\", \"answer_relevance\", \"faithfulness\"]\n",
    "}\n",
    "\n",
    "# Create directories if not exists\n",
    "os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mSample structure for evaluation_questions.json:\u001b[39m\n                                                   ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def load_evaluation_data(file_path):\n",
    "    \"\"\"Load evaluation questions with optional ground truth answers.\"\"\"\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return pd.DataFrame(data[\"questions\"])\n",
    "\n",
    "# Sample structure for evaluation_questions.json:\n",
    "# {\n",
    "#   \"questions\": [\n",
    "#     {\n",
    "#       \"question\": \"What are common credit card issues?\",\n",
    "#       \"ground_truth\": \"Customers report...\",\n",
    "#       \"expected_products\": [\"Credit card\"],\n",
    "#       \"expected_issues\": [\"Late fees\", \"Fraud\"]\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "eval_df = load_evaluation_data(CONFIG[\"eval_data_path\"])\n",
    "print(f\"Loaded {len(eval_df)} evaluation questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_pipeline(config):\n",
    "    \"\"\"Initialize all RAG pipeline components.\"\"\"\n",
    "    # Embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=config[\"embedding_model\"])\n",
    "    \n",
    "    # Vector Store\n",
    "    vectorstore = FAISS.load_local(config[\"vector_store_dir\"], embeddings)\n",
    "    \n",
    "    # LLM\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=config[\"llm_model\"],\n",
    "        model_kwargs={\"temperature\": 0.7, \"max_length\": 1024}\n",
    "    )\n",
    "    \n",
    "    # Prompt Template\n",
    "    prompt_template = \"\"\"\n",
    "    You are a financial analyst assistant. Answer the question based only on:\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Guidelines:\n",
    "    1. Be concise but comprehensive\n",
    "    2. If unsure, say \"I cannot determine from the context\"\n",
    "    3. Highlight patterns when apparent\n",
    "    4. Never invent information\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # QA Chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": config[\"retrieval_k\"]}),\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return {\"qa_chain\": qa_chain, \"vectorstore\": vectorstore}\n",
    "\n",
    "rag_components = initialize_rag_pipeline(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Comprehensive RAG system evaluator with multiple metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        self.evaluators = {\n",
    "            \"relevance\": load_evaluator(\"labeled_score_string\"),\n",
    "            \"faithfulness\": load_evaluator(\"labeled_score_string\")\n",
    "        }\n",
    "    \n",
    "    def calculate_retrieval_metrics(self, retrieved_docs, expected_products, expected_issues):\n",
    "        \"\"\"Calculate precision and recall for retrieved documents.\"\"\"\n",
    "        retrieved_products = [doc.metadata.get(\"product\", \"\") for doc in retrieved_docs]\n",
    "        retrieved_issues = [doc.metadata.get(\"issue\", \"\") for doc in retrieved_docs]\n",
    "        \n",
    "        # Binary relevance for each doc\n",
    "        product_relevance = [1 if p in expected_products else 0 for p in retrieved_products]\n",
    "        issue_relevance = [1 if i in expected_issues else 0 for i in retrieved_issues]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"retrieval_product_precision\": precision_score(product_relevance, [1]*len(product_relevance)),\n",
    "            \"retrieval_product_recall\": recall_score(product_relevance, [1]*len(product_relevance)),\n",
    "            \"retrieval_issue_precision\": precision_score(issue_relevance, [1]*len(issue_relevance)),\n",
    "            \"retrieval_issue_recall\": recall_score(issue_relevance, [1]*len(issue_relevance)),\n",
    "            \"num_relevant_docs\": sum(product_relevance)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_generation_metrics(self, generated_answer, ground_truth):\n",
    "        \"\"\"Calculate text generation quality metrics.\"\"\"\n",
    "        # ROUGE scores\n",
    "        rouge_scores = self.scorer.score(ground_truth, generated_answer)\n",
    "        \n",
    "        # BERTScore\n",
    "        _, _, bert_f1 = bert_score([generated_answer], [ground_truth], lang=\"en\")\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "            \"rougeL\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "            \"bert_score\": bert_f1.mean().item()\n",
    "        }\n",
    "    \n",
    "    def evaluate_response(self, question, generated_answer, retrieved_docs, ground_truth=None, expected_products=None, expected_issues=None):\n",
    "        \"\"\"Comprehensive evaluation of a RAG response.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Retrieval metrics\n",
    "        if expected_products and expected_issues:\n",
    "            metrics.update(self.calculate_retrieval_metrics(retrieved_docs, expected_products, expected_issues))\n",
    "        \n",
    "        # Generation metrics\n",
    "        if ground_truth:\n",
    "            metrics.update(self.calculate_generation_metrics(generated_answer, ground_truth))\n",
    "        \n",
    "        # LLM-based evaluations\n",
    "        if \"answer_relevance\" in CONFIG[\"evaluation_metrics\"]:\n",
    "            relevance_result = self.evaluators[\"relevance\"].evaluate_strings(\n",
    "                prediction=generated_answer,\n",
    "                input=question,\n",
    "                reference=ground_truth if ground_truth else \"\"\n",
    "            )\n",
    "            metrics[\"answer_relevance\"] = relevance_result[\"score\"]\n",
    "        \n",
    "        if \"faithfulness\" in CONFIG[\"evaluation_metrics\"]:\n",
    "            faithfulness_result = self.evaluators[\"faithfulness\"].evaluate_strings(\n",
    "                prediction=generated_answer,\n",
    "                input=\" \".join([doc.page_content for doc in retrieved_docs])\n",
    "            )\n",
    "            metrics[\"faithfulness\"] = faithfulness_result[\"score\"]\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "evaluator = RAGEvaluator(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(qa_chain, eval_df, evaluator):\n",
    "    \"\"\"Run full evaluation pipeline.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "        try:\n",
    "            # Get RAG response\n",
    "            result = qa_chain({\"query\": row[\"question\"]})\n",
    "            \n",
    "            # Evaluate response\n",
    "            metrics = evaluator.evaluate_response(\n",
    "                question=row[\"question\"],\n",
    "                generated_answer=result[\"result\"],\n",
    "                retrieved_docs=result[\"source_documents\"],\n",
    "                ground_truth=row.get(\"ground_truth\"),\n",
    "                expected_products=row.get(\"expected_products\", []),\n",
    "                expected_issues=row.get(\"expected_issues\", [])\n",
    "            )\n",
    "            \n",
    "            # Record results\n",
    "            record = {\n",
    "                \"question\": row[\"question\"],\n",
    "                \"generated_answer\": result[\"result\"],\n",
    "                \"ground_truth\": row.get(\"ground_truth\"),\n",
    "                \"retrieved_products\": [doc.metadata.get(\"product\") for doc in result[\"source_documents\"]],\n",
    "                \"retrieved_issues\": [doc.metadata.get(\"issue\") for doc in result[\"source_documents\"]],\n",
    "                **metrics\n",
    "            }\n",
    "            results.append(record)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question '{row['question']}': {str(e)}\")\n",
    "            results.append({\n",
    "                \"question\": row[\"question\"],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "results_df = run_evaluation(rag_components[\"qa_chain\"], eval_df, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results_df):\n",
    "    \"\"\"Generate comprehensive analysis of evaluation results.\"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # Basic stats\n",
    "    analysis[\"num_questions\"] = len(results_df)\n",
    "    analysis[\"success_rate\"] = 1 - (results_df[\"error\"].notna().sum() / len(results_df))\n",
    "    \n",
    "    # Metric averages\n",
    "    numeric_cols = results_df.select_dtypes(include=np.number).columns\n",
    "    analysis[\"metrics\"] = results_df[numeric_cols].mean().to_dict()\n",
    "    \n",
    "    # Retrieval analysis\n",
    "    all_products = [p for sublist in results_df[\"retrieved_products\"].dropna() for p in sublist]\n",
    "    analysis[\"top_retrieved_products\"] = pd.Series(all_products).value_counts().head(5).to_dict()\n",
    "    \n",
    "    all_issues = [i for sublist in results_df[\"retrieved_issues\"].dropna() for i in sublist]\n",
    "    analysis[\"top_retrieved_issues\"] = pd.Series(all_issues).value_counts().head(5).to_dict()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    if \"num_relevant_docs\" in results_df and \"bert_score\" in results_df:\n",
    "        analysis[\"relevance_correlation\"] = results_df[[\"num_relevant_docs\", \"bert_score\"]].corr().iloc[0,1]\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "analysis_results = analyze_results(results_df)\n",
    "print(\"Evaluation Analysis:\")\n",
    "print(json.dumps(analysis_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results_df, analysis):\n",
    "    \"\"\"Create interactive visualizations of evaluation results.\"\"\"\n",
    "    # Metric distribution\n",
    "    metric_cols = [col for col in results_df.columns if col in [\"rouge1\", \"rougeL\", \"bert_score\", \"answer_relevance\", \"faithfulness\"]]\n",
    "    \n",
    "    if metric_cols:\n",
    "        fig = px.box(results_df[metric_cols], \n",
    "                    title=\"Evaluation Metrics Distribution\")\n",
    "        fig.show()\n",
    "    \n",
    "    # Retrieval performance\n",
    "    if \"num_relevant_docs\" in results_df:\n",
    "        fig = px.histogram(results_df, x=\"num_relevant_docs\", \n",
    "                         title=\"Number of Relevant Documents Retrieved\")\n",
    "        fig.show()\n",
    "    \n",
    "    # Top retrieved products/issues\n",
    "    for entity_type in [\"products\", \"issues\"]:\n",
    "        data = analysis.get(f\"top_retrieved_{entity_type}\", {})\n",
    "        if data:\n",
    "            df = pd.DataFrame(list(data.items()), columns=[entity_type[:-1], \"count\"])\n",
    "            fig = px.bar(df, x=entity_type[:-1], y=\"count\", \n",
    "                        title=f\"Top Retrieved {entity_type.capitalize()}\")\n",
    "            fig.show()\n",
    "\n",
    "visualize_results(results_df, analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(results_df, analysis, config):\n",
    "    \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_path = f\"{config['results_dir']}/rag_evaluation_report_{timestamp}.html\"\n",
    "    \n",
    "    # Create report content\n",
    "    report_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>RAG Evaluation Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial; margin: 20px; }}\n",
    "            h1, h2 {{ color: #2e6c80; }}\n",
    "            .metric {{ background-color: #f2f2f2; padding: 10px; margin: 5px; }}\n",
    "            .good {{ color: green; }}\n",
    "            .bad {{ color: red; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>RAG System Evaluation Report</h1>\n",
    "        <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        \n",
    "        <h2>Summary Metrics</h2>\n",
    "        <div class=\"metric\">Questions Evaluated: {analysis['num_questions']}</div>\n",
    "        <div class=\"metric\">Success Rate: {analysis['success_rate']:.1%}</div>\n",
    "        \n",
    "        <h2>Performance Metrics</h2>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add metric cards\n",
    "    for metric, value in analysis[\"metrics\"].items():\n",
    "        rating_class = \"good\" if value > 0.7 else \"bad\" if value < 0.5 else \"\"\n",
    "        report_content += f\"<div class='metric {rating_class}'>{metric.replace('_', ' ').title()}: {value:.3f}</div>\"\n",
    "    \n",
    "    # Add detailed results\n",
    "    report_content += \"\"\"\n",
    "        <h2>Detailed Results</h2>\n",
    "        <table border=\"1\">\n",
    "            <tr>\n",
    "                <th>Question</th>\n",
    "                <th>BERT Score</th>\n",
    "                <th>Relevant Docs</th>\n",
    "                <th>Answer Preview</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        preview = row[\"generated_answer\"][:100] + \"...\" if pd.notna(row.get(\"generated_answer\")) else \"ERROR\"\n",
    "        bert_score = f\"{row['bert_score']:.3f}\" if 'bert_score' in row else \"N/A\"\n",
    "        relevant_docs = row.get('num_relevant_docs', \"N/A\")\n",
    "        \n",
    "        report_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{row['question']}</td>\n",
    "                <td>{bert_score}</td>\n",
    "                <td>{relevant_docs}</td>\n",
    "                <td>{preview}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    report_content += \"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save report\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "report_file = generate_report(results_df, analysis_results, CONFIG)\n",
    "print(f\"Report generated: {report_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_df, analysis, config):\n",
    "    \"\"\"Save all evaluation results for future reference.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"{config['results_dir']}/rag_results_{timestamp}.csv\"\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    \n",
    "    # Save analysis\n",
    "    analysis_file = f\"{config['results_dir']}/rag_analysis_{timestamp}.json\"\n",
    "    with open(analysis_file, \"w\") as f:\n",
    "        json.dump(analysis, f, indent=2)\n",
    "    \n",
    "    return results_file, analysis_file\n",
    "\n",
    "results_file, analysis_file = save_results(results_df, analysis_results, CONFIG)\n",
    "print(f\"Results saved to {results_file}\")\n",
    "print(f\"Analysis saved to {analysis_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(analysis):\n",
    "    \"\"\"Generate actionable recommendations based on evaluation results.\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Retrieval recommendations\n",
    "    if analysis[\"metrics\"].get(\"retrieval_product_precision\", 0) < 0.6:\n",
    "        recommendations.append({\n",
    "            \"category\": \"Retrieval\",\n",
    "            \"recommendation\": \"Improve retrieval precision by experimenting with different embedding models or adding query expansion\",\n",
    "            \"priority\": \"high\"\n",
    "        })\n",
    "    \n",
    "    # Generation recommendations\n",
    "    if analysis[\"metrics\"].get(\"bert_score\", 0) < 0.7:\n",
    "        recommendations.append({\n",
    "            \"category\": \"Generation\",\n",
    "            \"recommendation\": \"Refine prompt engineering or try different LLM models to improve answer quality\",\n",
    "            \"priority\": \"high\"\n",
    "        })\n",
    "    \n",
    "    # General recommendations\n",
    "    if analysis[\"success_rate\"] < 0.9:\n",
    "        recommendations.append({\n",
    "            \"category\": \"Robustness\",\n",
    "            \"recommendation\": \"Add better error handling for failed questions\",\n",
    "            \"priority\": \"medium\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "recommendations_df = generate_recommendations(analysis_results)\n",
    "display(recommendations_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
